{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Import all libraries\n",
    "\n",
    "*cv2*: OpenCV library, which provides tools for computer vision tasks such as image processing, feature detection, and object recognition.\n",
    "\n",
    "*numpy*: numerical computing, which is commonly used for data manipulation and array operations.\n",
    "\n",
    "*tensorflow*: Main library of machine learning for building and training deep learning models.\n",
    "\n",
    "*tensorflow_datasets*: Provides preprocessed datasets for machine learning tasks.\n",
    "\n",
    "*matplotlib*: Creating visualizations, which may be used for plotting model performance during training.\n",
    "\n",
    "*import of Dense, Conv2D, MaxPooling2D, Flatten, Dropout, and BatchNormalization*: These are Keras layers from TensorFlow that can be used to build neural networks for various machine learning tasks, more for image classification."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout, BatchNormalization"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Set hyperparameters, for affect model performance\n",
    "batch_size(128) - how much images the model will process at a time during each training iteration.\n",
    "epochs(30) - number of model iterations during training process\n",
    "Callback functions which used during training process:\n",
    "* tensorboard_callback - allows you to use TensorBoard, which is a visualization tool provided by TensorFlow, in web UI: http://localhost:6006/\n",
    "* stop_val_loss - monitors the validation loss and stops the training process if it does not improve for a certain number of epochs(30)\n",
    "* stop_val_accuracy - monitors the validation accuracy and stops the training process if it does not improve for a certain number of epochs(30)\n",
    "* lr_schedule_val_loss - monitors the validation loss and reduces the learning rate (lr) when it does not improve for a certain number of epochs(30)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset_name = 'cifar10'\n",
    "image_directory = 'random_images'\n",
    "batch_size = 128\n",
    "n_epochs = 30\n",
    "tensorboard_callback = TensorBoard(log_dir=f'logs/{dataset_name}')\n",
    "stop_val_loss = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=20)\n",
    "stop_val_accuracy = EarlyStopping(monitor='val_acc', mode='min', verbose=1, patience=20)\n",
    "lr_schedule_val_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "lr_schedule_val_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5)\n",
    "Load the CIFAR-10 dataset, which is a collection of 50,000 training images and 10,000 test images, where each image belongs to one of 10 classes. Link: https://paperswithcode.com/dataset/cifar-10\n",
    "\n",
    "Load data and split to train and test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "_, info = tfds.load(dataset_name, split=['train', 'test'], with_info=True)\n",
    "(train_data, train_labels), (test_data, test_labels) = cifar10.load_data()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Get parameters of data info:\n",
    "* class names of dataset CIFAR-10\n",
    "* number of classes (10)\n",
    "* shape of images from dataset (32, 32, 3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class_names = info.features[\"label\"].names\n",
    "n_classes = info.features[\"label\"].num_classes\n",
    "input_shape = info.features['image'].shape\n",
    "\n",
    "print(f\"Size of classes: {n_classes}, and their names is: {class_names}\")\n",
    "print(f\"Shape of data: {input_shape}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Visualisation of CIFAR-10 data.\n",
    "Generates a grid of 16 random images with their corresponding labels. Take 16 images from train_data. For each image, its corresponding label is used to set the title."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "random_nums = [random.randint(0, 50000) for _ in range(16)]\n",
    "\n",
    "for random, i in zip(random_nums, range(16)):\n",
    "  plt.subplot(4, 4, i+1)\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])\n",
    "  plt.imshow(train_data[random], cmap=plt.cm.binary)\n",
    "  plt.xlabel(class_names[train_labels[random][0]])\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Normalizing data for model trainings:\n",
    "Pixel values of the training and test data are rescaled to a range of [0, 1] by dividing them by 255\n",
    "\n",
    "Flattening the label arrays for ensures that they are in the appropriate format for feeding into the model during training, where the model expects a 1D array of labels."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_data = train_data / 255\n",
    "test_data = test_data / 255\n",
    "\n",
    "train_labels, test_labels = train_labels.flatten(), test_labels.flatten()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Data augmentation for better training process:\n",
    "Used to artificially increase the size of the training dataset by applying random transformations to the existing images. It helps in improving the model's ability to generalize and handle variations in the input data.\n",
    "Different parameters of ImageDataGenerator:\n",
    "* rotation_range: Specifies the range of random rotations\n",
    "* zoom_range: Controls the range of random zooming\n",
    "* width_shift_range and height_shift_range: Determine the range of random horizontal and vertical shifts"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=15,\n",
    "    zoom_range=0.1,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=False\n",
    ")\n",
    "\n",
    "train_generator = datagen.flow(train_data, train_labels, batch_size=batch_size)\n",
    "test_generator = datagen.flow(test_data, test_labels, batch_size=batch_size)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Convolutional Neural Network model creating\n",
    "\n",
    "Sequential model that consists of several layers:\n",
    "\n",
    "* `Conv2D`: Performs 2D convolution on the input data. Takes 32, 64 or 128 number of filters, filter/kernel size (3x3 in this case), the 'same' padding argument ensures that the output feature map has the same spatial dimensions as the input feature map. The activation function used is ReLU(simple non-linear function that takes the maximum between the input value and 0).\n",
    "* `MaxPooling2D`: Performs max pooling on the output of the previous Conv2D layer. The pool size is (2, 2), which reduces the spatial dimensions of the feature map by half.\n",
    "* `BatchNormalization`: Normalize the outputs of a previous layer. It works by normalizing the inputs of each mini-batch during training. The purpose is to improve the stability and performance of the model.\n",
    "* `Dropout`: Randomly drops out a certain proportion of neurons of the input units to prevent overfitting.\n",
    "* `Flatten`: Flattens the output of the previous layer to a 1D array to prepare it for the fully connected layers.\n",
    "* `Dense`: Fully connected layer with X units(512 and 1024 in this case) and uses the ReLU activation function. The last Dense layer has number of classes units and uses the softmax activation function to produce class probabilities."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Conv2D(32, 3, padding='same', input_shape=input_shape, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(32, 3, activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2, 2)),\n",
    "\n",
    "    Conv2D(64, 3, padding='same', activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(64, 3, activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2, 2)),\n",
    "\n",
    "    Conv2D(128, 3, padding='same', activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(128, 3, activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2, 2)),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(1024, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(n_classes, activation='softmax'),\n",
    "])\n",
    "\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Compile model with specific parameters:\n",
    "* `Adam optimizer`: combines the benefits of two other popular optimization methods(AdaGrad and RMSProp). It adapts the learning rate dynamically for each parameter, which allows for efficient and effective optimization of the model's weights during training\n",
    "* `Sparse Categorical Cross-Entropy loss function`: specifically designed for multi-class classification problems(like this) where the labels are integers. It calculates the difference between the predicted probability distribution and the true label distribution. This loss function appropriate because the labels in the dataset are integers representing the class categories."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Train model (first process)\n",
    "Used train data and test data like validation, used callbacks that initialized in beginning\n",
    "Steps per epoch - this specifies the number of steps (batches) to be processed in each epoch. It is typically set as the total number of training samples(50000 and 10000) divided by the batch size(128)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "history = model.fit(train_data, train_labels, epochs=n_epochs,\n",
    "                 validation_data=(test_data, test_labels),\n",
    "                 callbacks=[tensorboard_callback, lr_schedule_val_loss, stop_val_loss, stop_val_accuracy],\n",
    "                 batch_size=batch_size,\n",
    "                 steps_per_epoch=len(train_data) // batch_size,\n",
    "                 validation_steps=len(test_data) // batch_size)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Train model (second process)\n",
    "If in first process model can be little bit overfitted then for improve training process, using data augmentation which was done previously and then starts training process"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "history2 = model.fit(train_generator, epochs=n_epochs,\n",
    "                 validation_data=test_generator,\n",
    "                 callbacks=[tensorboard_callback, lr_schedule_val_loss, stop_val_loss, stop_val_accuracy],\n",
    "                 batch_size=batch_size,\n",
    "                 steps_per_epoch=len(train_data) // batch_size,\n",
    "                 validation_steps=len(test_data) // batch_size)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Evaluate model on test data\n",
    "\n",
    "Performs forward propagation on the test data through the trained model and computes the loss and accuracy metrics."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(test_data, test_labels)\n",
    "print(f\"Loss: {loss}, Accuracy: {accuracy}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Visualizing the performance of a deep learning CNN model\n",
    "Visualize data, that have been got after training process of model. Visulization of loss and accuracy line charts. During data watching can be identified some overfitting or underfitting of model\n",
    "\n",
    "\n",
    "Also for take a look for model training proccesses you can go to terminal execute *tensorboard --logdir=logs/* and go to http://localhost:6006/ , this is user interface of model training with TensorBoard"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(16, 8))\n",
    "\n",
    "ax1.plot(history.history['loss'], 'red', linewidth=2.0)\n",
    "ax1.plot(history.history['val_loss'], 'orange', linewidth=2.0)\n",
    "ax1.plot(history2.history['loss'], 'blue', linewidth=2.0)\n",
    "ax1.plot(history2.history['val_loss'], 'cyan', linewidth=2.0)\n",
    "ax1.legend(['[Simple data] Training Loss', '[Simple data] Validation Loss', '[Data augmentation] Training Loss', '[Data augmentation] Validation Loss'], fontsize=12)\n",
    "ax1.set_xlabel('Epochs', fontsize=14)\n",
    "ax1.set_ylabel('Loss', fontsize=14)\n",
    "ax1.set_title('Loss Curves', fontsize=16)\n",
    "ax1.grid(True)\n",
    "\n",
    "ax2.plot(history.history['acc'], 'red', linewidth=2.0)\n",
    "ax2.plot(history.history['val_acc'], 'orange', linewidth=2.0)\n",
    "ax2.plot(history2.history['acc'], 'blue', linewidth=2.0)\n",
    "ax2.plot(history2.history['val_acc'], 'cyan', linewidth=2.0)\n",
    "ax2.legend(['[Simple data] Training Accuracy', '[Simple data] Validation Accuracy', '[Data augmentation] Training Accuracy', '[Data augmentation] Validation Accuracy'], fontsize=12)\n",
    "ax2.set_xlabel('Epochs', fontsize=14)\n",
    "ax2.set_ylabel('Accuracy', fontsize=14)\n",
    "ax2.set_title('Accuracy Curves', fontsize=16)\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Visualize the model's predictions on a set of test data\n",
    "\n",
    "Iterates over the first 25 images in the test data and plots them on each subplot. Set title of true data class and predictioned class"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pred = model.predict(test_data)\n",
    "pred_classes = np.argmax(pred, axis=1)\n",
    "\n",
    "fig, axes = plt.subplots(5, 5, figsize=(15,15))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i in np.arange(0, 25):\n",
    "    axes[i].imshow(test_data[i])\n",
    "    axes[i].set_title(\"True: %s \\nPredict: %s\" % (class_names[test_labels[i]], class_names[pred_classes[i]]))\n",
    "    axes[i].axis('off')\n",
    "    plt.subplots_adjust(wspace=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Method for getting top prediction classes and names on some predictioned image, for visualizing this data on bar chart"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_top_predictions(prediction):\n",
    "    top_probs_indices = np.argsort(prediction)[0][::-1][:10]\n",
    "    top_probs = prediction[0][top_probs_indices]\n",
    "    top_class_names = [class_names[i] for i in top_probs_indices]\n",
    "    return top_class_names, top_probs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data prediction with random images(data)\n",
    "\n",
    "Loops over some random images, load it with OpenCV, resizing to 32x32, then make prediction with model and ploting the original image and the corresponding probabilities of each class(with method get_top_predictions)\n",
    "\n",
    "The resulting figure shows the original image with its predicted class and a bar chart of the probabilities for the top predicted classes."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for filename in os.listdir(image_directory):\n",
    "  if filename.endswith('.jpg') or filename.endswith('.png') or filename.endswith('.jpeg'):\n",
    "    plot_img = cv.imread(f\"{image_directory}/{filename}\")\n",
    "    plot_img = cv.cvtColor(plot_img, cv.COLOR_BGR2RGB)\n",
    "\n",
    "    img = cv.imread(f\"{image_directory}/{filename}\")\n",
    "    img = cv.resize(img, (32, 32))\n",
    "    img = np.expand_dims(img, axis=0) / 255.0\n",
    "\n",
    "    prediction = model.predict(img)\n",
    "    index = np.argmax(prediction)\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(8, 4))\n",
    "\n",
    "    ax1.set_title(f\"Truly is {filename.split('.')[0]}\\nPrediction is {class_names[index]}\")\n",
    "    ax1.imshow(plot_img)\n",
    "    ax1.axis(\"off\")\n",
    "\n",
    "    top_names, top_predictions = get_top_predictions(prediction)\n",
    "    ax2.barh(top_names, top_predictions)\n",
    "    ax2.set_xlabel(\"Probability\")\n",
    "    ax2.set_xlim(0, 1)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
